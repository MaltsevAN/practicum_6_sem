{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Decorators (10 points)\n",
    "1. Write a decorator, which checks complience of function's signature to given interface\n",
    "2. Write a decorator, which decorates function with a provided decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Signature(function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if len(inspect.getfullargspec(function)[0]) == len(args):\n",
    "            result = function(*args, **kwargs)\n",
    "            return result\n",
    "        else:\n",
    "            print('Signature error')\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator(fun):\n",
    "    @functools.wraps(fun)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return fun(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@check_Signature\n",
    "def foo1(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature error\n"
     ]
    }
   ],
   "source": [
    "foo1(3, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo1(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator(check_Signature)\n",
    "def foo2(a, b):\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo2(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature error\n"
     ]
    }
   ],
   "source": [
    "foo2(1,2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Text preprocessing (20 points)\n",
    "Assume we have an arbitrary text and plan to prepare it for a further data analysis. The text might contain html tags, emails, latex commands. The task is to create a configurable text preprocessor, which allows to remove certain elements from provided text. The following filters should be supported:\n",
    "1. Removal of punctuation symbols from a given list \n",
    "2. Removal of stop words from a given list\n",
    "3. Removal of HTML tags\n",
    "4. Removal of e-mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html_tag: https://tutorialedge.net/python/removing-html-from-string/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop_words:\n",
    "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emails: \n",
    "https://stackoverflow.com/questions/44027943/python-regex-to-remove-emails-from-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punctuation: https://stackoverflow.com/questions/4371231/removing-punctuation-from-python-list-items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexmal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_preprocessing():\n",
    "    def __init__(self, text,\n",
    "                 html = True,\n",
    "                 email = True,\n",
    "                 stop_words = True,\n",
    "                 punctuation = True,\n",
    "                 emails_tag = None,\n",
    "                 html_tags = None,\n",
    "                 stop_word_lang = 'english'):\n",
    "        self.text = text\n",
    "        self.html = html\n",
    "        self.email = email\n",
    "        self.stop_words = stop_words\n",
    "        self.punctuation = punctuation\n",
    "        self.without_stop_words = []\n",
    "        if html_tags is None:\n",
    "            self.html_tags = re.compile(r'<[^>]+>')\n",
    "        else:\n",
    "            self.html_tags = html_tags\n",
    "        if emails_tag is None:\n",
    "            self.emails_tag = re.compile(r'\\S*@\\S*\\s?')\n",
    "        else:\n",
    "            self.emails_tag = emails_tag\n",
    "        self.word_tokens = None\n",
    "        self.stop_words = None\n",
    "        self.stop_word_lang = stop_word_lang\n",
    "    def remove_punctuation_symbols(self):\n",
    "        if self.word_tokens is None:\n",
    "            self.word_tokens = word_tokenize(self.text)\n",
    "        self.without_punctuation = [''.join(c for c in s if c not in string.punctuation)\n",
    "                                    for s in self.word_tokens]\n",
    "        self.word_tokens = self.without_punctuation\n",
    "        self.text = ' '.join(self.without_punctuation)\n",
    "        return self.without_punctuation\n",
    "    def remove_stop_words(self):\n",
    "        if self.word_tokens is None:\n",
    "            self.word_tokens = word_tokenize(self.text)\n",
    "        for token in self.word_tokens:\n",
    "            if not token in self.stop_words:\n",
    "                self.without_stop_words.append(token)\n",
    "        self.word_tokens = self.without_stop_words\n",
    "        self.text = ' '.join(self.without_stop_words)\n",
    "        return self.without_stop_words\n",
    "    def remove_tags(self):\n",
    "        self.without_tags = self.html_tags.sub('', self.text)\n",
    "        self.text = self.without_tags\n",
    "        return self.without_tags\n",
    "    def remove_emails(self):\n",
    "        self.stop_words = set(stopwords.words(self.stop_word_lang))\n",
    "        self.without_emails = self.emails_tag.sub('', self.text)\n",
    "        self.text = self.without_emails\n",
    "        return self.without_emails\n",
    "    def preprocessing(self):\n",
    "        if self.html:\n",
    "            self.remove_tags()\n",
    "        if self.email:\n",
    "            self.remove_emails()\n",
    "        if self.stop_words:\n",
    "            self.remove_stop_words()\n",
    "        if self.punctuation:\n",
    "            self.remove_punctuation_symbols()\n",
    "        return self.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 'abc user@xxx.com 123 any@www foo ... ///  @ bar 78@ppp @5555 aa@111'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = '<a href=\"http://example.com/\">\\nI linked to <i>example.com</i>\\n</a>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = inp + ' ' + example_sent + ' ' + markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Text_preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': True,\n",
       " 'emails_tag': re.compile(r'\\S*@\\S*\\s?', re.UNICODE),\n",
       " 'html': True,\n",
       " 'html_tags': re.compile(r'<[^>]+>', re.UNICODE),\n",
       " 'punctuation': True,\n",
       " 'stop_word_lang': 'english',\n",
       " 'stop_words': None,\n",
       " 'text': 'abc user@xxx.com 123 any@www foo ... ///  @ bar 78@ppp @5555 aa@111 This is a sample sentence, showing off the stop words filtration. <a href=\"http://example.com/\">\\nI linked to <i>example.com</i>\\n</a>',\n",
       " 'without_stop_words': [],\n",
       " 'word_tokens': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc 123 foo   bar This sample sentence  showing stop words filtration  I linked examplecom'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email': True,\n",
       " 'emails_tag': re.compile(r'\\S*@\\S*\\s?', re.UNICODE),\n",
       " 'html': True,\n",
       " 'html_tags': re.compile(r'<[^>]+>', re.UNICODE),\n",
       " 'punctuation': True,\n",
       " 'stop_word_lang': 'english',\n",
       " 'stop_words': {'a',\n",
       "  'about',\n",
       "  'above',\n",
       "  'after',\n",
       "  'again',\n",
       "  'against',\n",
       "  'ain',\n",
       "  'all',\n",
       "  'am',\n",
       "  'an',\n",
       "  'and',\n",
       "  'any',\n",
       "  'are',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'as',\n",
       "  'at',\n",
       "  'be',\n",
       "  'because',\n",
       "  'been',\n",
       "  'before',\n",
       "  'being',\n",
       "  'below',\n",
       "  'between',\n",
       "  'both',\n",
       "  'but',\n",
       "  'by',\n",
       "  'can',\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'd',\n",
       "  'did',\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'do',\n",
       "  'does',\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'doing',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'down',\n",
       "  'during',\n",
       "  'each',\n",
       "  'few',\n",
       "  'for',\n",
       "  'from',\n",
       "  'further',\n",
       "  'had',\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'has',\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'have',\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'having',\n",
       "  'he',\n",
       "  'her',\n",
       "  'here',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'him',\n",
       "  'himself',\n",
       "  'his',\n",
       "  'how',\n",
       "  'i',\n",
       "  'if',\n",
       "  'in',\n",
       "  'into',\n",
       "  'is',\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'just',\n",
       "  'll',\n",
       "  'm',\n",
       "  'ma',\n",
       "  'me',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'more',\n",
       "  'most',\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'my',\n",
       "  'myself',\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'now',\n",
       "  'o',\n",
       "  'of',\n",
       "  'off',\n",
       "  'on',\n",
       "  'once',\n",
       "  'only',\n",
       "  'or',\n",
       "  'other',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'out',\n",
       "  'over',\n",
       "  'own',\n",
       "  're',\n",
       "  's',\n",
       "  'same',\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'so',\n",
       "  'some',\n",
       "  'such',\n",
       "  't',\n",
       "  'than',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'the',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'them',\n",
       "  'themselves',\n",
       "  'then',\n",
       "  'there',\n",
       "  'these',\n",
       "  'they',\n",
       "  'this',\n",
       "  'those',\n",
       "  'through',\n",
       "  'to',\n",
       "  'too',\n",
       "  'under',\n",
       "  'until',\n",
       "  'up',\n",
       "  've',\n",
       "  'very',\n",
       "  'was',\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'we',\n",
       "  'were',\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'what',\n",
       "  'when',\n",
       "  'where',\n",
       "  'which',\n",
       "  'while',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'why',\n",
       "  'will',\n",
       "  'with',\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\",\n",
       "  'y',\n",
       "  'you',\n",
       "  \"you'd\",\n",
       "  \"you'll\",\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves'},\n",
       " 'text': 'abc 123 foo   bar This sample sentence  showing stop words filtration  I linked examplecom',\n",
       " 'without_emails': 'abc 123 foo ... ///  bar This is a sample sentence, showing off the stop words filtration. \\nI linked to example.com\\n',\n",
       " 'without_punctuation': ['abc',\n",
       "  '123',\n",
       "  'foo',\n",
       "  '',\n",
       "  '',\n",
       "  'bar',\n",
       "  'This',\n",
       "  'sample',\n",
       "  'sentence',\n",
       "  '',\n",
       "  'showing',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'filtration',\n",
       "  '',\n",
       "  'I',\n",
       "  'linked',\n",
       "  'examplecom'],\n",
       " 'without_stop_words': ['abc',\n",
       "  '123',\n",
       "  'foo',\n",
       "  '...',\n",
       "  '///',\n",
       "  'bar',\n",
       "  'This',\n",
       "  'sample',\n",
       "  'sentence',\n",
       "  ',',\n",
       "  'showing',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'filtration',\n",
       "  '.',\n",
       "  'I',\n",
       "  'linked',\n",
       "  'example.com'],\n",
       " 'without_tags': 'abc user@xxx.com 123 any@www foo ... ///  @ bar 78@ppp @5555 aa@111 This is a sample sentence, showing off the stop words filtration. \\nI linked to example.com\\n',\n",
       " 'word_tokens': ['abc',\n",
       "  '123',\n",
       "  'foo',\n",
       "  '',\n",
       "  '',\n",
       "  'bar',\n",
       "  'This',\n",
       "  'sample',\n",
       "  'sentence',\n",
       "  '',\n",
       "  'showing',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'filtration',\n",
       "  '',\n",
       "  'I',\n",
       "  'linked',\n",
       "  'examplecom']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = 'Кулити работяги всем доброго здоровья держите рофл: \"Все, кто пропустил церемонию инаугурации презедента России Владимира Путина, не расстраивайтесь. Посмотрите в следующий раз.\" хахаха,ну це таке рофл годный, а новые мемасы спрашивайте по адресу vc.com/alexapitaine или по почте best_rofl_capitaine@gmail.com PS Некоторые знаки припинания сам по безграмотнотси удалил'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кулити работяги всем доброго здоровья держите рофл: \"Все, кто пропустил церемонию инаугурации презедента России Владимира Путина, не расстраивайтесь. Посмотрите в следующий раз.\" хахаха,ну це таке рофл годный, а новые мемасы спрашивайте по адресу vc.com/alexapitaine или по почте best_rofl_capitaine@gmail.com PS Некоторые знаки припинания сам по безграмотнотси удалил'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Text_preprocessing(text_2, stop_word_lang='russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кулити работяги всем доброго здоровья держите рофл   Все  пропустил церемонию инаугурации презедента России Владимира Путина  расстраивайтесь  Посмотрите следующий   хахаха  це таке рофл годный  новые мемасы спрашивайте адресу vccomalexapitaine почте PS Некоторые знаки припинания безграмотнотси удалил'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
